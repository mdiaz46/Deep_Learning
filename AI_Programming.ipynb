{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9xRRL+gGoEalwUbyVU1X3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mdiaz46/Deep_Learning/blob/main/AI_Programming.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Marlon Diaz 06/07/2024\n",
        "# DEEP LEARNING\n",
        "\n",
        "## A simple Python example with a single neuron. It has two inputs and one output and executes the logic AND of the inputs.\n",
        "\n",
        "The logic AND of the inputs in the provided code can be found `in the training section` of the code where the neurons is trained based on the given inouts and outputs.\n",
        "\n",
        "- The inputs are defined as a 2D array with four sets of input combinations: [[0,0], [1,1], [1,0], [0,1]].\n",
        "\n",
        "- The corresponding outputs for these inputs are defined as another array: [[0,1,1,1]].T (transpose of the array to align with the inputs).\n",
        "\n",
        "- Weights are initialized randomly with values between -1 and 1.\n",
        "\n",
        "- The `neuron` function takes inputs and weights as parameters and calculates the output using the sigmoid activation function.\n",
        "\n",
        "- The neuron is trained for 50000 iterations by adjusting the weights based on the error between the expected outputs and the actual output of the neuron.\n",
        "\n",
        "- Finally, a test input array [1,0] is passed to the neuron function along with the trained weights to get the output.\n",
        "\n",
        "The logic AND operation in this code is represented by the training process where the neural network is being trained to approximate the AND function. The inputs [1,1], [1,0], and [0,1] are mapped to the output 1, representing the logical AND operation for these input combinations. The aim of training the neuron is to minimize the error between  the expected output (based on the AND logic) and the output produced by the neuron.\n",
        "\n",
        "The output result of 0.9968177 you are seeing when running the code indicates the result of passing the input [1, 0] through the trained neuron with the weights calculated during the training process.\n",
        "\n",
        "In the code snippet you provided, the input array x is defined as [1, 0]. This input is then passed to the `neuron` function along with the weights obtained after training the neuron for 50000 iterations.\n",
        "\n",
        "The `neuron` function calculates the output by applying the sigmoid activation function to the dot product of the input array and the weights. The output value of 0.9968177 represents the output of the neuron when the input [1, 0] is passed through it using the trained weights.\n",
        "\n",
        "This output value of 0.9968177 indicates that for the input [1, 0], the neuron is predicting a value very close to 1, which aligns with the logic AND operation being approximated by the trained neural network. In the context of the AND logic, the output of 0.9968177 suggests that the neuron is correctly predicting the logical AND operation for the input [1, 0], where the expected output is 1.\n",
        "\n",
        "# Notes on the Sigmoid Activation Function\n",
        "\n",
        "The sigmoid activation function is a mathematical function that maps any real value to a value between 0 and 1. It is commonly used in neural networks to introduce non-linearity into the output of a neuron. The sigmoid function is defined as:\n",
        "\n",
        "$ { \\displaystyle f(x) = \\frac{1}{1 + e^{-x}}}$\n",
        "\n",
        "In the provided code snippet, the sigmoid activation function is used in the `neuron` function to calculate the output of the neuron. The sigmoid function is applied to the result of the dot product of the inputs and weights.\n",
        "\n",
        "Here is the snippet of code where the sigmoid activation function is used:\n",
        "\n",
        "```python\n",
        "def neuron(inputs, weights):\n",
        "    output = 1 / (1 + exp(-(dot(inputs, weights))))\n",
        "    return output\n",
        "```\n",
        "\n",
        "In this code snippet:\n",
        "- The `neuron` function takes the inputs and weights as parameters.\n",
        "- The output of the neuron is calculated by applying the sigmoid activation function to the result of the dot product of inputs and weights.\n",
        "- The sigmoid activation function is implemented as 1 / (1 + exp(-(dot(inputs, weights)))), where `exp` is the exponential function from numpy that calculates e to the power of the negative argument.\n",
        "\n",
        "Therefore, the `neuron` function in the provided code snippet implements the sigmoid activation function to introduce non-linearity into the output of the neuron. The output value returned by the `neuron` function is the result of applying the sigmoid function to the dot product of inputs and weights, representing the activation output of the neuron for a given set of inputs and weights."
      ],
      "metadata": {
        "id": "w5ltLN9se3pR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Example Neuron.py\n",
        "\n",
        "from numpy import exp, array, random, dot\n",
        "\n",
        "#Set up parameters ===============================================\n",
        "inputs = array([[0, 0], [1, 1], [1, 0], [0, 1]])\n",
        "outputs = array([[0, 1, 1, 1]]).T\n",
        "random.seed(1)\n",
        "weights = 2 * random.random((2, 1)) - 1\n",
        "\n",
        "#Define a Single Neuron function =================================\n",
        "def neuron(inputs, weights):\n",
        "    output = 1 / (1 + exp(-(dot(inputs, weights))))\n",
        "    return output\n",
        "\n",
        "#Train the Neuron ================================================\n",
        "for iteration in range(50000):\n",
        "    output = 1 / (1 + exp(-(dot(inputs, weights))))\n",
        "    weights += dot(inputs.T, (outputs - output) * output * (1 - output))\n",
        "\n",
        "#Test the Neuron =================================================\n",
        "x = array([1, 0])\n",
        "print (neuron (x,weights))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7hGvUZ-ev3q",
        "outputId": "70c956b4-e6d2-4dca-de47-381481cdecce"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.9968177]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P_6k650aEAl2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}